# ğŸ“„ CV Assistant â€” RAG + Groq + Pinecone + Streamlit

Un **chatbot inteligente** que responde preguntas sobre mi perfil profesional usando **RAG (Retrieval-Augmented Generation)**, un LLM acelerado por **Groq**, y un Ã­ndice vectorial en **Pinecone**.

Funciona como una versiÃ³n conversacional de mi CV, ideal para reclutadores, entrevistas tÃ©cnicas y networking profesional.

ğŸ‘‰ **Demo en vivo**: *PrÃ³ximamente*

---

## ğŸ¯ Â¿QuÃ© hace este proyecto?

Este asistente de CV permite a cualquier persona hacer preguntas sobre mi experiencia profesional, habilidades, formaciÃ³n y proyectos de forma natural y conversacional. El sistema recupera informaciÃ³n relevante de mi CV y la presenta de manera clara y profesional, evitando alucinaciones gracias al uso de RAG.

### Ejemplo de uso:
- **Pregunta:** "Â¿QuÃ© experiencia tenÃ©s en Machine Learning?"
- **Respuesta:** InformaciÃ³n precisa extraÃ­da del CV sobre proyectos, herramientas y aÃ±os de experiencia.

---

## âœ¨ CaracterÃ­sticas principales

- ğŸ” **RAG real**: Las respuestas provienen de informaciÃ³n autÃ©ntica del CV (sin inventar datos)
- âš¡ **LLM ultrarrÃ¡pido**: Usa Groq con modelos Llama optimizados para latencias muy bajas (<1s)
- ğŸ“š **Vector Database**: Almacenamiento eficiente de embeddings en Pinecone (serverless)
- ğŸ“ **Metadata enriquecida**: InformaciÃ³n estructurada como nombre, email, skills, experiencia, etc.
- ğŸ¨ **Interfaz moderna**: UI profesional construida con Streamlit
- ğŸ§  **Memoria de sesiÃ³n**: Mantiene el contexto de la conversaciÃ³n
- ğŸš« **Sin alucinaciones**: Reglas estrictas para evitar informaciÃ³n inventada
- ğŸ“¸ **Header personalizado**: Incluye foto personal y diseÃ±o profesional

---

## ğŸ—ï¸ Arquitectura del proyecto

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario       â”‚
â”‚   (Pregunta)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI              â”‚
â”‚   (rag_app.py)              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º 1. Embed pregunta (Sentence Transformers)
       â”‚
       â”œâ”€â”€â–º 2. Buscar contexto en Pinecone (top-k chunks)
       â”‚
       â”œâ”€â”€â–º 3. Cargar metadata (metadata.json)
       â”‚
       â””â”€â”€â–º 4. Generar respuesta con Groq (Llama 3.1)
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Respuesta      â”‚
       â”‚   (sin inventar) â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo RAG:
1. **Ingesta** (`rag_ingest.py`): El CV en PDF se divide en chunks, se generan embeddings y se almacenan en Pinecone
2. **Consulta** (`rag_app.py`): 
   - La pregunta del usuario se convierte en embedding
   - Se buscan los chunks mÃ¡s relevantes en Pinecone
   - Se construye un prompt con metadata + chunks recuperados
   - Groq genera una respuesta natural basada Ãºnicamente en esa informaciÃ³n

---

## ğŸ› ï¸ Stack tecnolÃ³gico

| Componente | TecnologÃ­a | PropÃ³sito |
|------------|-----------|-----------|
| **Frontend** | Streamlit | Interfaz web interactiva |
| **LLM** | Groq (Llama 3.1 8B Instant) | GeneraciÃ³n de respuestas en lenguaje natural |
| **Vector DB** | Pinecone (Serverless) | Almacenamiento y bÃºsqueda de embeddings |
| **Embeddings** | Sentence Transformers (`all-MiniLM-L6-v2`) | ConversiÃ³n de texto a vectores |
| **PDF Processing** | pdfplumber | ExtracciÃ³n de texto del CV |
| **Lenguaje** | Python 3.9+ | Backend y procesamiento |

---

## ğŸ“¦ InstalaciÃ³n y configuraciÃ³n

### 1ï¸âƒ£ Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/TP2-NLP2.git
cd TP2-NLP2
```

### 2ï¸âƒ£ Crear entorno virtual

```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Instalar dependencias

```bash
pip install -r rag/requirements.txt
```

### 4ï¸âƒ£ Configurar variables de entorno

Copia el archivo `.env.example` a `.env` y completa con tus credenciales:

```bash
cp .env.example .env
```

Edita `.env` con tus claves:

```env
PINECONE_API_KEY=tu_clave_de_pinecone
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1
PINECONE_INDEX=cv-alumno
GROQ_API_KEY=tu_clave_de_groq
```

**DÃ³nde obtener las claves:**
- **Pinecone**: [Registrate aquÃ­](https://www.pinecone.io/) (plan gratuito disponible)
- **Groq**: [Consigue tu API key](https://console.groq.com/) (gratuito, muy generoso)

### 5ï¸âƒ£ Preparar tu CV

1. Coloca tu CV en PDF en `docs/Tu_Nombre_CV.pdf`
2. Coloca tu foto en `docs/foto.jpg` (o actualiza la ruta en `rag_app.py`)
3. Edita `docs/metadata.json` con tu informaciÃ³n personal

Ejemplo de `metadata.json`:

```json
{
  "nombre": "Tu Nombre",
  "titulo": "Data Scientist",
  "profesion": "CientÃ­fico de Datos",
  "ubicacion": "Buenos Aires, Argentina",
  "fecha_nacimiento": "2000-01-01",
  "email": "tu@email.com",
  "linkedin": "linkedin.com/in/tu-perfil",
  "nivel_ingles": "Avanzado (C1)",
  "seniority": "Semi Senior",
  "experiencia_anios": 3,
  "skills_clave": ["Python", "Machine Learning", "SQL"]
}
```

---

## ğŸš€ Uso

### Paso 1: Ingestar el CV en Pinecone

Antes de usar el chatbot, debes procesar tu CV y subirlo a Pinecone:

```bash
python rag/rag_ingest.py
```

Esto:
- Lee tu CV en PDF
- Lo divide en chunks inteligentes
- Genera embeddings con Sentence Transformers
- Sube todo a Pinecone

**Salida esperada:**
```
âœ… Cliente Pinecone inicializado correctamente
âœ… Modelo de embeddings cargado (384 dimensiones)
ğŸ“„ Texto extraÃ­do del PDF (5432 caracteres)
âœ‚ï¸ Generados 12 chunks
ğŸš€ Iniciando ingesta de 12 chunks...
   âœ 12/12 chunks subidos
ğŸ‰ Ingesta completada correctamente
```

### Paso 2: Lanzar la aplicaciÃ³n Streamlit

```bash
streamlit run rag/rag_app.py
```

La aplicaciÃ³n se abrirÃ¡ automÃ¡ticamente en `http://localhost:8501`

### Paso 3: Â¡Empezar a conversar! ğŸ’¬

Haz preguntas como:
- "Â¿QuÃ© experiencia tenÃ©s en NLP?"
- "Contame sobre tus proyectos de Machine Learning"
- "Â¿QuÃ© herramientas de MLOps manejÃ¡s?"
- "Â¿CuÃ¡l es tu nivel de inglÃ©s?"

---

## ğŸ“ Estructura del proyecto

```
TP2-NLP2/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Abril Noguera - CV.pdf    # CV en formato PDF
â”‚   â”œâ”€â”€ foto.jpg                   # Foto personal para el header
â”‚   â””â”€â”€ metadata.json              # InformaciÃ³n estructurada del CV
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag_ingest.py             # Script de ingesta a Pinecone
â”‚   â”œâ”€â”€ rag_app.py                # AplicaciÃ³n Streamlit principal
â”‚   â”œâ”€â”€ validate_env.py           # Validador de variables de entorno
â”‚   â””â”€â”€ requirements.txt          # Dependencias del proyecto
â”‚
â”œâ”€â”€ .env.example                   # Plantilla de variables de entorno
â”œâ”€â”€ .gitignore                     # Archivos ignorados por Git
â””â”€â”€ README.md                      # Este archivo
```

---

## ğŸ”§ ConfiguraciÃ³n avanzada

### Ajustar parÃ¡metros de RAG

En `rag_app.py` puedes modificar:

```python
# NÃºmero de chunks recuperados
def retrieve(question: str, top_k: int = 5):  # Aumenta para mÃ¡s contexto

# ParÃ¡metros del LLM
resp = client.chat.completions.create(
    model="llama-3.1-8b-instant",
    temperature=0.2,        # Creatividad (0-1)
    max_tokens=600,         # Longitud mÃ¡xima de respuesta
)
```

### Cambiar modelo de embeddings

En `rag_ingest.py` y `rag_app.py`:

```python
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"  # RÃ¡pido y ligero
# Alternativas:
# "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # Mejor multiidioma
# "sentence-transformers/all-mpnet-base-v2"  # Mejor calidad, mÃ¡s lento
```

### Chunking personalizado

En `rag_ingest.py`:

```python
def chunkear_texto(texto: str, max_chars=700, overlap=100):
    # max_chars: TamaÃ±o de cada chunk
    # overlap: Solapamiento entre chunks (evita perder contexto)
```

---

## ğŸ¨ PersonalizaciÃ³n de la UI

### Cambiar colores y estilos

Edita el CSS en `rag_app.py`:

```python
st.markdown("""
<style>
body { background-color: #0f172a; }  /* Fondo oscuro */
.chat-user { background:#334155; }    /* Mensajes del usuario */
.chat-bot { background:#1e293b; }     /* Mensajes del bot */
</style>
""", unsafe_allow_html=True)
```

### Modificar header

```python
st.markdown("<h1 class='header-name'>Tu Nombre</h1>", unsafe_allow_html=True)
st.markdown("<div class='header-sub'>Tu tÃ­tulo profesional</div>", unsafe_allow_html=True)
```

---

## ğŸ§ª ValidaciÃ³n de entorno

Para verificar que todas las variables de entorno estÃ¡n configuradas correctamente:

```bash
python rag/validate_env.py
```

---

## ğŸš¨ SoluciÃ³n de problemas comunes

### Error: "PINECONE_API_KEY no estÃ¡ configurada"

**SoluciÃ³n:** AsegÃºrate de tener un archivo `.env` en la raÃ­z del proyecto con todas las claves necesarias.

### Error: "No se pudo extraer texto del PDF"

**SoluciÃ³n:** Tu PDF puede ser una imagen escaneada. NecesitarÃ¡s un PDF con texto seleccionable o usar OCR.

### La aplicaciÃ³n no muestra respuestas

**SoluciÃ³n:** 
1. Verifica que ejecutaste `rag_ingest.py` primero
2. Confirma que el Ã­ndice de Pinecone tiene datos:
   ```python
   from pinecone import Pinecone
   pc = Pinecone(api_key="tu_clave")
   index = pc.Index("cv-alumno")
   print(index.describe_index_stats())
   ```

### Groq responde muy lento

**SoluciÃ³n:** Groq es extremadamente rÃ¡pido. Si hay lentitud, probablemente sea tu conexiÃ³n a internet o lÃ­mites de tasa (espera unos segundos y reintenta).

---

## ğŸ“Š Consideraciones tÃ©cnicas

### Embeddings

- **Modelo**: `all-MiniLM-L6-v2` (384 dimensiones)
- **Ventajas**: RÃ¡pido, ligero, bueno para espaÃ±ol e inglÃ©s
- **Desventajas**: Para CVs muy tÃ©cnicos, considera modelos mÃ¡s grandes

### LLM (Groq)

- **Modelo**: Llama 3.1 8B Instant
- **Latencia**: ~200-500ms por respuesta
- **LÃ­mites**: ~30 req/min en plan gratuito (muy generoso para este uso)

### Pinecone

- **Plan gratuito**: 1 Ã­ndice, hasta 100k vectores (suficiente para decenas de CVs)
- **Latencia**: ~50-100ms por query
- **Escalabilidad**: Serverless â†’ se ajusta automÃ¡ticamente

### Costos estimados

- **Pinecone Free**: $0/mes (hasta 100k vectores)
- **Groq**: $0/mes (lÃ­mites generosos)
- **Total**: **GRATIS** para uso personal

---

## ğŸ” Seguridad y privacidad

- âœ… Las claves API se manejan mediante variables de entorno
- âœ… El archivo `.env` estÃ¡ en `.gitignore` (no se sube a Git)
- âœ… Los datos del CV se almacenan en tu instancia de Pinecone (privada)
- âš ï¸ **Importante**: No compartas tu archivo `.env` ni lo subas a repositorios pÃºblicos

---

## ğŸ¤ Contribuciones

Este es un proyecto acadÃ©mico/personal, pero si encontrÃ¡s bugs o mejoras:

1. **ReportÃ¡ issues** en GitHub
2. **Propone mejoras** via Pull Requests
3. **Comparte tu feedback** en LinkedIn

---

## ğŸ“š Recursos adicionales

- [DocumentaciÃ³n de Pinecone](https://docs.pinecone.io/)
- [Groq API Reference](https://console.groq.com/docs)
- [Sentence Transformers](https://www.sbert.net/)
- [Streamlit Docs](https://docs.streamlit.io/)
- [RAG Explained](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

## ğŸ“ Licencia

Este proyecto fue desarrollado como parte del **TP2 de Procesamiento de Lenguaje Natural (NLP2)**.

**Autor**: Abril Noguera  
**Contacto**: abrilnoguera@gmail.com  
**LinkedIn**: [linkedin.com/in/abrilnoguera](https://linkedin.com/in/abrilnoguera)

---

## ğŸ“ CrÃ©ditos acadÃ©micos

**Materia**: Procesamiento de Lenguaje Natural 2  
**Trabajo PrÃ¡ctico**: TP2 - RAG Application  
**AÃ±o**: 2025

